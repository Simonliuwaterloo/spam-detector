{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building NB algorithm by hand\n",
    "## 1.1 data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "\n",
    "def import_data():\n",
    "    ham_list = []\n",
    "    spam_list = []\n",
    "\n",
    "    for filename in os.listdir(\"./data/email/ham/\"):\n",
    "        f = open(\"./data/email/ham/\" + filename, mode=\"r\", encoding=\"latin-1\")\n",
    "        ham_list.append(f.read())\n",
    "\n",
    "    for filename in os.listdir(\"./data/email/spam/\"):\n",
    "        f = open(\"./data/email/spam/\" + filename, mode=\"r\", encoding=\"latin-1\")\n",
    "        spam_list.append(f.read())\n",
    "    return ham_list, spam_list  \n",
    "\n",
    "def textParse(bigString):\n",
    "    listOfTokens = re.split('\\W+', bigString)\n",
    "    regex = re.compile('[0-9]+')  # filter out words with numbers\n",
    "    return [tok.lower() for tok in listOfTokens if (len(tok) > 2 and not regex.search(tok))]   \n",
    "\n",
    "def createDataSet(ham_list, spam_list):\n",
    "    train_ham_list = ham_list[:200]\n",
    "    test_ham_list = ham_list[200:400]\n",
    "    train_spam_list = spam_list[:50]\n",
    "    test_spam_list = spam_list[50:100]\n",
    "    listOfPost = []\n",
    "    listOfClass = []\n",
    "    listOfTestPost = []\n",
    "    listOfTestClass = []\n",
    "    for post in train_ham_list:\n",
    "        listOfPost.append(textParse(post))\n",
    "        listOfClass.append(0) # 0 is ham, 1 is spam\n",
    "    for post in train_spam_list:\n",
    "        listOfPost.append(textParse(post))\n",
    "        listOfClass.append(1)\n",
    "    for post in test_ham_list:\n",
    "        listOfTestPost.append(textParse(post))\n",
    "        listOfTestClass.append(0) \n",
    "    for post in test_spam_list:\n",
    "        listOfTestPost.append(textParse(post))\n",
    "        listOfTestClass.append(1)\n",
    "    return listOfPost, listOfClass, listOfTestPost, listOfTestClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_list, spam_list = import_data()\n",
    "listOfPost, listOfClass,listOfTestPost, testClass = createDataSet(ham_list, spam_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. build classifier\n",
    "### 1.2.1 create vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocab(listOfPost):\n",
    "    vocabList = []\n",
    "    for post in listOfPost:\n",
    "        vocabList.extend(post)\n",
    "    return list(set(vocabList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = createVocab(listOfPost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 convert documents to word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def doc2WordVec(post, vocab):\n",
    "    retvec = np.zeros(len(vocab))\n",
    "    for word in post:\n",
    "        if word in vocab:\n",
    "            retvec[vocab.index(word)] += 1\n",
    "    return retvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training matrix\n",
    "trainMat = np.zeros((0, len(vocab)))\n",
    "for doc in listOfPost:\n",
    "    trainMat = np.append(trainMat, doc2WordVec(doc, vocab)[np.newaxis,:], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 calculate probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainMat, listOfClass):\n",
    "    numSpam = np.sum(listOfClass)\n",
    "    numWords = len(trainMat[0])\n",
    "    numDocs = len(trainMat)\n",
    "    p1 = numSpam/numDocs\n",
    "    p0Vec = np.ones(numWords); p1Vec = np.ones(numWords)\n",
    "    p0Denom = 2; p1Denom = 2\n",
    "    for i in range(numDocs):\n",
    "        if listOfClass[i] == 1:\n",
    "            p1Vec += trainMat[i]\n",
    "            p1Denom += np.sum(trainMat[i])\n",
    "        else:\n",
    "            p0Vec += trainMat[i]\n",
    "            p0Denom += np.sum(trainMat[i])  \n",
    "    p0Vec = np.log(p0Vec/p0Denom)\n",
    "    p1Vec = np.log(p1Vec/p1Denom)\n",
    "    return p0Vec, p1Vec, p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pSpam = trainNB(trainMat, listOfClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.54684337, -11.38005671,  -9.58829724, ..., -11.38005671,\n",
       "       -11.38005671, -10.68690953])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 make classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a diffeent way to convert from words to vector\n",
    "# set word to 1, on occurance\n",
    "def doc2WordVec1(post, vocab): # todo: change bad naming\n",
    "    retvec = np.zeros(len(vocab))\n",
    "    for word in post:\n",
    "        if word in vocab:\n",
    "            retvec[vocab.index(word)] = 1\n",
    "    return retvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(vector2Classify, p0V, p1V, pSpam):\n",
    "    probSpam = np.sum(doc2WordVec1(vector2Classify, vocab)*p1V) + np.log(pSpam)\n",
    "    probHam = np.sum(doc2WordVec1(vector2Classify, vocab)*p0V) + np.log(1 - pSpam)\n",
    "    if probSpam > probHam:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(testData, testClass, p0V, p1V, pSpam):\n",
    "    testResult = [];\n",
    "    for i in range(len(testData)):\n",
    "        testResult.append(classify(testData[i], p0V, p1V, pSpam) == testClass[i])\n",
    "    return testResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = testModel(listOfTestPost, testClass, p0V, p1V, pSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using small data set for now because this algorithm is slow\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count(1)/len(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
